{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EARTH Part 2 - Dump the model to Pytorch Mobile",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeTUonJr_zwd"
      },
      "source": [
        "# DUMP the model to PM, allowing for it to run on Android Devices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7d8JqJn_wEO",
        "outputId": "dcb0e32c-45f4-462f-d946-567c5c03dd8e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cXGwDIvFtLq"
      },
      "source": [
        "%%capture\n",
        "!kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-buSxZVFvad"
      },
      "source": [
        "!cp -f kaggle.json /root/.kaggle/kaggle.json\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KThWlMvKFxCf"
      },
      "source": [
        "%%capture\n",
        "!kaggle datasets download -d kneroma/tacotrashdataset -p ../input/tacotrashdataset\n",
        "!unzip ../input/tacotrashdataset/tacotrashdataset.zip -d ../input/tacotrashdataset/ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcVXB_5RGSyE"
      },
      "source": [
        "# Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2cstG_3GUgW"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxPvF8RpF8-c"
      },
      "source": [
        "Data Dir"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsMyKgVlF91z"
      },
      "source": [
        "class DataModule:\n",
        "  base_dir = './drive/MyDrive/EARTH_Models/'\n",
        "  csv_file = '../input/tacotrashdataset/meta_df.csv'\n",
        "  csv_module = pd.read_csv(csv_file)\n",
        "  ALL_CLASSES = sorted(csv_module.cat_name.unique())\n",
        "\n",
        "  data_csv = '../input/tacotrashdataset/meta_df.csv' # All the Data inside of TacoTrashDataset\n",
        "  # Needs to be split.\n",
        "  data_df = pd.read_csv(data_csv)\n",
        "  # Change the paths to the BASE DATA DIR\n",
        "  BASE_DATA_DIR = '../input/tacotrashdataset/data/'\n",
        "  data_df['img_file'] = BASE_DATA_DIR + data_df['img_file']\n",
        "\n",
        "  # ---------BASIC DATA PREP--------------\n",
        "  # Convert the Multiple Rows of the DF into a single Caption\n",
        "  all_unique_ids = data_df.img_file.unique()\n",
        "  # One-Hot Encode classes\n",
        "  classes2idx = {}\n",
        "  idx2classes = {}\n",
        "  ALL_CLASSES = sorted(data_df.cat_name.unique())\n",
        "  for idx, class_name in enumerate(ALL_CLASSES):\n",
        "    idx2classes[idx] = class_name\n",
        "    classes2idx[class_name] = idx\n",
        "  NUM_CLASSES = len(classes2idx)\n",
        "  \n",
        "  PAD_BOUNDING_BOXES = -100 # Pad with < -1. This Ignores the Regression Targets.\n",
        "\n",
        "  TARGET_DIR = './drive/MyDrive/EARTH_Models/'"
      ],
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfEUXq1IHehL"
      },
      "source": [
        "Pickle the classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_y7ctuSHeHn"
      },
      "source": [
        "with open(f\"{DataModule.base_dir}classes.txt\", 'w') as file:\n",
        "  file.write(str(DataModule.ALL_CLASSES))"
      ],
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwQg_E43_38N"
      },
      "source": [
        "# Step 3: Prepare the model for direct inference\n",
        "- Instead of outputting logits, output the Bounding Boxes themselves\n",
        "- Jit Compile, collapsing this into a function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SLIY2j4tmdF"
      },
      "source": [
        "%%capture\n",
        "!wget https://github.com/rwightman/efficientdet-pytorch/archive/refs/heads/master.zip\n",
        "!unzip ./master.zip\n",
        "!rm -f ./master.zip"
      ],
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpqV2Ln3ttNY"
      },
      "source": [
        "%%capture \n",
        "!pip install pycocotools\n",
        "!pip install timm\n",
        "!pip install omegaconf"
      ],
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZKbEC0ztxQ2"
      },
      "source": [
        "%%capture\n",
        "%cd efficientdet-pytorch-master/\n",
        "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n",
        "from effdet.efficientdet import HeadNet\n",
        "import effdet\n",
        "%cd .."
      ],
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8vUx8zutyDI"
      },
      "source": [
        "%%capture\n",
        "import math\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "!pip install pytorch_lightning\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4VBgYGMxfYx"
      },
      "source": [
        "from effdet import *"
      ],
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCQT_bBiuZv0"
      },
      "source": [
        "# Create the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tby7b1vnwmV0"
      },
      "source": [
        "class TRAININGOBJECT(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()    \n",
        "    self.model = get_net()"
      ],
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHPuHcWtwhpS"
      },
      "source": [
        "class PyTorchLightningModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.model = self.configure_model()\n",
        "\n",
        "\n",
        "    self.train_loss = 0.0\n",
        "    self.train_steps = 0\n",
        "    self.val_loss = 0.0\n",
        "    self.val_steps = 0\n",
        "  def configure_model(self):\n",
        "    model = TRAININGOBJECT()\n",
        "    return model \n",
        "  def configure_optimizers(self):\n",
        "    optimizer = optim.AdamW(self.logger_object.training_object.model.parameters(), \n",
        "        lr = self.logger_object.training_object.lr,\n",
        "        weight_decay = self.logger_object.training_object.weight_decay)\n",
        "    \n",
        "    lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 5, eta_min = 1e-13, verbose = True)\n",
        "    return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler}\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    images, bboxes, classification = batch\n",
        "    images = images.half()\n",
        "    loss = train_step(images, bboxes, classification, training_object=self.logger_object.training_object)\n",
        "    self.train_loss += loss.item()\n",
        "    self.train_steps += 1\n",
        "    return loss\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    images, bboxes, classification = batch\n",
        "    images = images.half()\n",
        "    loss = val_step(images, bboxes, classification, training_object=self.logger_object.training_object)\n",
        "    self.val_loss += loss.item()\n",
        "    self.val_steps += 1\n",
        "  def validation_epoch_end(self, logs):\n",
        "    eps = 1e-10\n",
        "    self.logger_object.update_states((self.train_loss + eps) / (self.train_steps + eps), (self.val_loss + eps) / (self.val_steps + eps))\n",
        "    self.train_loss = 0.\n",
        "    self.train_steps = 0.\n",
        "    self.val_loss = 0.\n",
        "    self.val_steps = 0.0\n"
      ],
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyvAqhDpxPA2"
      },
      "source": [
        "IMG_SIZE = 512"
      ],
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPb8l4h2vNlw"
      },
      "source": [
        "class PyTorchLightningModelWrapper(nn.Module):\n",
        "  def __init__(self, model):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "  def forward(self, *args, **kwargs):\n",
        "    return self.model(*args, **kwargs)\n",
        "def get_net():\n",
        "    config = get_efficientdet_config('tf_efficientdet_d0') # Smallest Model Possible, to make it runable on CPU(Especially an Android CPU)\n",
        "    net = EfficientDet(config, pretrained_backbone=False)\n",
        "\n",
        "    effdet.config.config_utils.set_config_writeable(config) \n",
        "    config.num_classes = DataModule.NUM_CLASSES\n",
        "    config.image_size = (IMG_SIZE, IMG_SIZE)\n",
        "    effdet.config.config_utils.set_config_readonly(config)\n",
        "    net.class_net = HeadNet(config, num_outputs=config.num_classes)\n",
        "    model = DetBenchTrain(net, config)\n",
        "    return PyTorchLightningModelWrapper(model)\n"
      ],
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYE0nG9nvYBh"
      },
      "source": [
        "class InferenceModel(nn.Module):\n",
        "  def __init__(self, prev_model):\n",
        "    super().__init__()\n",
        "    self.prev_model = prev_model \n",
        "    self.model = PyTorchLightningModel()\n",
        "    self.model.model.model.model.load_state_dict(torch.load(self.prev_model, map_location = device))\n",
        "    self.model = DetBenchPredict(self.model.model.model.model.model).to(device)\n",
        "  def forward(self, x):\n",
        "    self.eval()\n",
        "    B = x.shape[0]\n",
        "    with torch.no_grad():\n",
        "      x = x.to(device)\n",
        "      predictions =  self.model(x, img_info = {'img_scale': torch.tensor([1.0] * B, dtype=torch.float).to(device), \n",
        "      'img_size':  torch.tensor([x[0].shape[-2:]] * B, \n",
        "      dtype=torch.float).to(device)})\n",
        "      # Hard NMS\n",
        "      idx = torchvision.ops.nms(predictions[0, :, :4], predictions[0, :, 4], iou_threshold = 0.5)\n",
        "      predictions = predictions[0, idx]\n",
        "      return predictions\n",
        "    "
      ],
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6HCmA-wwTMy"
      },
      "source": [
        "model = InferenceModel(DataModule.base_dir + 'final.pth')"
      ],
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh6tLBBox2Kf"
      },
      "source": [
        "# Serialize the model\n",
        "- Model Output: Tensor(N,6): (x1, y1, x2, y2, obj, cls)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y2ALTj675iV"
      },
      "source": [
        "from torch.utils.mobile_optimizer import optimize_for_mobile"
      ],
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZbQFnpHwWXY",
        "outputId": "2323df3d-06db-40ed-b0e7-d5bc72019c08"
      },
      "source": [
        "# Trace the Model, send it to torch.mobile\n",
        "example = torch.rand(1, 3, IMG_SIZE, IMG_SIZE)\n",
        "traced_script_module = torch.jit.trace(model, example)\n",
        "traced_script_module_optimized = optimize_for_mobile(traced_script_module)\n",
        "traced_script_module_optimized._save_for_lite_interpreter(\"./model.ptl\")\n"
      ],
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.7/dist-packages/timm/models/layers/padding.py:19: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  return max((math.ceil(x / s) - 1) * s + (k - 1) * d + 1 - x, 0)\n",
            "/usr/local/lib/python3.7/dist-packages/timm/models/layers/padding.py:19: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  return max((math.ceil(x / s) - 1) * s + (k - 1) * d + 1 - x, 0)\n",
            "/usr/local/lib/python3.7/dist-packages/timm/models/layers/padding.py:31: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if pad_h > 0 or pad_w > 0:\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX4tXfxcxl-d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}