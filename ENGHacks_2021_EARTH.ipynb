{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ENGHacks 2021 - EARTH",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvr6ftMmcbak"
      },
      "source": [
        "# EARTH: **E**nvironmental **A**i **R**ubbish de**T**ection tec**H**nology."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ITI6apv-3nr"
      },
      "source": [
        "# What the Application can do:\n",
        "- Segments Garbage inside of Images\n",
        "   - Could be used in autonomous robot system\n",
        "- Logs Litter Consumption/Production inside of geographic locations, to organize citywide cleanups\n",
        "  - Displays a heatmap of litter inside of canadian cities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DerpAU_qDgk0"
      },
      "source": [
        "# Load Drive "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NvBvg4sDAdv",
        "outputId": "d03b8fa4-70d0-42c2-db9c-13e3afbf8d3f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2nssVzmcYxv"
      },
      "source": [
        "# Download the EfficientDet Library\n",
        "- Pytorch EfficientDet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMkkj95Fdd2q"
      },
      "source": [
        "%%capture\n",
        "!wget https://github.com/rwightman/efficientdet-pytorch/archive/refs/heads/master.zip\n",
        "!unzip ./master.zip\n",
        "!rm -f ./master.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dACBQMgSEIce"
      },
      "source": [
        "# Download the TACO Dataset:\n",
        "- Luckily, the TACO dataset is hosted on Kaggle(Oct 2020 - so a bit outdated)\n",
        "- Pipeline: \n",
        "  - YOLOv5 Model\n",
        "  - Ported to PyTorch Mobile\n",
        "  - Hosted on Android Studio, for mobile detection(For Future Robotics and Autonomous garbage cleanup robots)\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcMP_fDMDuop"
      },
      "source": [
        "%%capture\n",
        "!kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "corEoL0CDxzm"
      },
      "source": [
        "!cp -f kaggle.json /root/.kaggle/kaggle.json\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNsN5x2hD2Bi"
      },
      "source": [
        "%%capture\n",
        "!kaggle datasets download -d kneroma/tacotrashdataset -p ../input/tacotrashdataset\n",
        "!unzip ../input/tacotrashdataset/tacotrashdataset.zip -d ../input/tacotrashdataset/ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9QQ71MQEjCf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxe3X3RdePBu"
      },
      "source": [
        "# install EfficientDet Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZwTSkzreSbi"
      },
      "source": [
        "%%capture \n",
        "!pip install pycocotools\n",
        "!pip install timm\n",
        "!pip install omegaconf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucyFj8BGjOH2"
      },
      "source": [
        "%%capture\n",
        "%cd efficientdet-pytorch-master/\n",
        "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n",
        "from effdet.efficientdet import HeadNet\n",
        "import effdet\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juwmNxJbGS9e"
      },
      "source": [
        "# Import Dependencies "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hghu18iyJhcG"
      },
      "source": [
        "%%capture \n",
        "!pip install --upgrade albumentations==0.5.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suOcypUUGUze",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31046d2e-0882-4d15-8510-62fc529eca85"
      },
      "source": [
        "import math\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "!pip install pytorch_lightning\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.7/dist-packages (1.3.7.post0)\n",
            "Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.4.1)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.41.1)\n",
            "Requirement already satisfied: PyYAML<=5.4.1,>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (5.4.1)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2021.6.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (20.9)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.18.2)\n",
            "Requirement already satisfied: pyDeprecate==0.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.3.0)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.9.0+cu102)\n",
            "Requirement already satisfied: torchmetrics>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.3.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (57.0.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.31.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.12.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.12.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.36.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.34.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.8.0)\n",
            "Requirement already satisfied: aiohttp; extra == \"http\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.7.4.post0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytorch_lightning) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->pytorch_lightning) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (2021.5.30)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (4.5.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.6.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (5.1.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyIVx9VvGRnS"
      },
      "source": [
        "# Process the Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHlFx-uHFf_Z"
      },
      "source": [
        "class DataModule:\n",
        "  data_csv = '../input/tacotrashdataset/meta_df.csv' # All the Data inside of TacoTrashDataset\n",
        "  # Needs to be split.\n",
        "  data_df = pd.read_csv(data_csv)\n",
        "  # Change the paths to the BASE DATA DIR\n",
        "  BASE_DATA_DIR = '../input/tacotrashdataset/data/'\n",
        "  data_df['img_file'] = BASE_DATA_DIR + data_df['img_file']\n",
        "\n",
        "  # ---------BASIC DATA PREP--------------\n",
        "  # Convert the Multiple Rows of the DF into a single Caption\n",
        "  all_unique_ids = data_df.img_file.unique()\n",
        "  # One-Hot Encode classes\n",
        "  classes2idx = {}\n",
        "  idx2classes = {}\n",
        "  ALL_CLASSES = sorted(data_df.cat_name.unique())\n",
        "  for idx, class_name in enumerate(ALL_CLASSES):\n",
        "    idx2classes[idx] = class_name\n",
        "    classes2idx[class_name] = idx\n",
        "  NUM_CLASSES = len(classes2idx)\n",
        "  \n",
        "  PAD_BOUNDING_BOXES = -100 # Pad with < -1. This Ignores the Regression Targets.\n",
        "\n",
        "  TARGET_DIR = './drive/MyDrive/EARTH_Models/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYIolnoA-1D0"
      },
      "source": [
        "# EXPORT THE CLASSES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jog18-nW_JQT"
      },
      "source": [
        "import json\n",
        "with open(f\"{DataModule.TARGET_DIR}classes.json\", \"w\") as file:\n",
        "  json.dump(DataModule.idx2classes, file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FwHlHK45UPf"
      },
      "source": [
        "def yolo_encode(x, y, width, height, img_width, img_height, class_idx):\n",
        "  x1 = x\n",
        "  x2 = x + width\n",
        "  y1 = y \n",
        "  y2 = y + height\n",
        "\n",
        "  cx = int((x1 + x2) / 2) \n",
        "  cy = int((y1 + y2) / 2)\n",
        "  # Normalize By IMG_WIDTH, and IMG_HEIGHT\n",
        "  cx = cx / img_width\n",
        "  cy = cy / img_height\n",
        "\n",
        "  width = width / img_width\n",
        "  height = height / img_height\n",
        "  return np.array([cx, cy, width, height, class_idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Swikcmr4YGH"
      },
      "source": [
        "def xyxy_encode(x, y, width, height, class_idx):\n",
        "  x0 = x\n",
        "  y0 = y\n",
        "\n",
        "  x1 = x0 + width\n",
        "  y1 = y0 + height\n",
        "  return np.array([x0, y0, x1, y1, class_idx]) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymybwelUxWVA"
      },
      "source": [
        "# Convert the DF into a (cx, cy, w, h, cls) -> (cx / width, cy / height, w / width, h / height, cls)\n",
        "# Bounding boxes are converted to relative ones in the EffDet module\n",
        "# relative Bounding Boxes: (ax - cx, ay - cy, log(aw / w), log(ay / h), cls)\n",
        "\n",
        "GT = {}\n",
        "for id in DataModule.all_unique_ids:\n",
        "  all_lines = DataModule.data_df[DataModule.data_df['img_file'] == id]\n",
        "  bbox_annotations = []\n",
        "  for line in all_lines.iterrows():\n",
        "    line = line[1]\n",
        "    x = line.x\n",
        "    y = line.y\n",
        "    img_width = line.img_width\n",
        "    img_height = line.img_height\n",
        "\n",
        "    width = line.width\n",
        "    height = line.height\n",
        "    annotation_id = DataModule.classes2idx[line.cat_name]\n",
        "\n",
        "    bbox = xyxy_encode(x, y, width, height, annotation_id) \n",
        "    bbox_annotations += [bbox]\n",
        "  bbox_annotations = np.stack(bbox_annotations)\n",
        "  GT[id] = bbox_annotations\n",
        "keys = np.array(tuple(GT))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNIyeAeySyAK",
        "outputId": "6de5d2c6-cd1d-4407-da1d-f0e6e1f7b00f"
      },
      "source": [
        "# Compute Max number of bboxes(to Pad to) - ~100 Bboxes max(After Mosaic + Cutout, it's 600)\n",
        "max_num = 0.0\n",
        "for key in GT.keys():\n",
        "  valid_rows = DataModule.data_df[DataModule.data_df.img_file == key]\n",
        "  if len(valid_rows) > max_num:\n",
        "    max_num = len(valid_rows)\n",
        "print(max_num) \n",
        "MAX_NUMBER = 100 # 100 Bounding Boxes ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "90\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh6BV34J_dLl"
      },
      "source": [
        "# Data Splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dnlw_gbDBhwz"
      },
      "source": [
        "TRAIN_DATA = {}\n",
        "TRAIN_KEYS = []\n",
        "VAL_DATA = {}\n",
        "VAL_KEYS = []\n",
        "\n",
        "splitter = ShuffleSplit(n_splits = 1, test_size = 0.1, train_size = 0.9, random_state = 42)\n",
        "\n",
        "for train_idx, test_idx in splitter.split(keys):\n",
        "\n",
        "  train_keys = keys[train_idx]\n",
        "  test_keys = keys[test_idx]\n",
        "\n",
        "  for key in train_keys:\n",
        "    TRAIN_DATA[key] = GT[key]\n",
        "    TRAIN_KEYS.append(key.item())\n",
        "  for key in test_keys:\n",
        "    VAL_DATA[key] = GT[key]\n",
        "    VAL_KEYS.append(key.item())\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sq3JVAH7ZE6"
      },
      "source": [
        "class PadBBoxes(object):\n",
        "  # Pads the Bounding Boxes.\n",
        "  def __init__(self):\n",
        "    self.num_bboxes = MAX_NUMBER\n",
        "  def __call__(self, images, bboxes, classification):\n",
        "    # Pads the bounding boxes with -1 obj score.\n",
        "    # bboxes: Tensor(N, 5)\n",
        "    # images: Tensor(C, H, W)\n",
        "    # PAD THE NUMBER OF BOUNDING BOXES to 100.\n",
        "    N = bboxes.shape[0]\n",
        "    num_pad = self.num_bboxes - N # (100 - N, 5)\n",
        "    pad_bboxes = torch.zeros((num_pad, 4), dtype = bboxes.dtype)\n",
        "    bboxes = torch.cat([bboxes, pad_bboxes], dim = 0)\n",
        "\n",
        "    pad_classification = torch.zeros((num_pad), dtype = classification.dtype) + DataModule.PAD_BOUNDING_BOXES\n",
        "    classification = torch.cat([classification, pad_classification], dim = 0) # (N, )\n",
        "    return images, bboxes, classification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJtFHwWy-lZ-"
      },
      "source": [
        "# Torch Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6W6OaLTInnu"
      },
      "source": [
        "# Augmentations\n",
        "IMG_SIZE = 512 # This will be deployed on a mobile device, we need the smallest models possible.\n",
        "data_augmentations = A.Compose([\n",
        "      A.RandomResizedCrop(IMG_SIZE, IMG_SIZE),\n",
        "      A.OneOf([\n",
        "          A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
        "                                val_shift_limit=0.2, p=0.9),\n",
        "          A.RandomBrightnessContrast(brightness_limit=0.2, \n",
        "                                      contrast_limit=0.2, p=0.9),\n",
        "      ],p=0.9),\n",
        "      A.ToGray(p=0.1),\n",
        "      A.HorizontalFlip(p=0.5),\n",
        "      A.VerticalFlip(p=0.5),\n",
        "      A.RandomRotate90(p=0.5),\n",
        "  A.Transpose(p=0.5),\n",
        "  A.JpegCompression(quality_lower=85, quality_upper=95, p=0.2),\n",
        "  A.OneOf([\n",
        "  A.Blur(blur_limit=3, p=1.0),\n",
        "  A.MedianBlur(blur_limit=3, p=1.0)\n",
        "  ],p=0.1),\n",
        "  A.Cutout(num_holes=8, max_h_size=16, max_w_size=16, fill_value=0, p=0.5),\n",
        "  ToTensorV2(p=1.0),   \n",
        "], bbox_params = A.BboxParams('pascal_voc', label_fields = ['classes']))\n",
        "\n",
        "test_augmentations = A.Compose([\n",
        "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
        "    ToTensorV2()\n",
        "], bbox_params = A.BboxParams('pascal_voc', label_fields = ['classes']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97P-geAJcTea"
      },
      "source": [
        "class ConvertToBBoxes(object):\n",
        "  def __init__(self):\n",
        "    self.num_classes = DataModule.NUM_CLASSES\n",
        "  def __call__(self, images, bboxes):\n",
        "    # images: tensor(B, C, H, W)\n",
        "    # bboxes: tensor(B, N, 5)\n",
        "    obj_idx = bboxes[:, -1].to(torch.int64) # (N, )\n",
        "    \n",
        "    bbox_reg = bboxes[:, :-1] # (N, 4)\n",
        "    return bbox_reg, obj_idx    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TldRIU1zJc81"
      },
      "source": [
        "class Mixup(object):\n",
        "  # 1x1 Mixup.\n",
        "  def __init__(self, p = 0.5):\n",
        "    self.p = p\n",
        "  def __call__(self, images, bounding_boxes, classification):\n",
        "    # Images: Tensor(B, C, H, W)\n",
        "    # Bounding Boxes: Tensor(B, N, 4)\n",
        "    # (B, N, 5) ->(cx, cy, w, h, obj score) \n",
        "    B, C, H, W = images.shape\n",
        "    new_images = []\n",
        "    new_bounding_boxes = []\n",
        "    new_classification = []\n",
        "    for b in range(B):\n",
        "      # Select One Other Bounding Box\n",
        "      if random.random() > self.p:\n",
        "        new_images += [images[b]]\n",
        "        base_bbox = bounding_boxes[b]\n",
        "        base_class_scores = classification[b]\n",
        "        bboxes = torch.cat([base_bbox, base_bbox], dim = 0) \n",
        "        new_bounding_boxes += [bboxes]\n",
        "\n",
        "        padded = torch.ones_like(base_class_scores) * DataModule.PAD_BOUNDING_BOXES\n",
        "        new_cls = torch.cat([base_class_scores, padded], dim = 0)\n",
        "        \n",
        "        new_classification += [new_cls]\n",
        "        continue \n",
        "\n",
        "      else:\n",
        "        idx = random.randint(0, B - 1) # idx\n",
        "        base_image = images[b] # (C, H, W)\n",
        "        base_bounding_boxes = bounding_boxes[b] # (N, 5)\n",
        "        base_classification = classification[b]\n",
        "\n",
        "        second_image = images[idx] # (C, H, W)\n",
        "        second_bounding_boxes = bounding_boxes[idx] # (N, 5)\n",
        "        second_classification = classification[idx]\n",
        "\n",
        "        # --------AVERAGE THE IMAGES-----------------\n",
        "        new_image = (base_image + second_image) / 2 # (C, H, W)\n",
        "        new_BBOX = torch.cat([base_bounding_boxes, second_bounding_boxes], dim = 0) # (2N, 5)\n",
        "        new_CLASSES = torch.cat([base_classification, second_classification], dim = 0)\n",
        "\n",
        "\n",
        "        new_images += [new_image]\n",
        "        new_bounding_boxes += [new_BBOX]\n",
        "        new_classification += [new_CLASSES] \n",
        "\n",
        "    new_images = torch.stack(new_images, dim = 0)\n",
        "    new_bounding_boxes = torch.stack(new_bounding_boxes, dim = 0)\n",
        "    new_classification = torch.stack(new_classification, dim = 0)\n",
        "    return new_images, new_bounding_boxes, new_classification\n",
        "class Mosaic(object):\n",
        "  def __init__(self, p = 0.5):\n",
        "    self.p = p\n",
        "  def __call__(self, images, bounding_boxes, classification):\n",
        "    # Images: Tensor(B, C, H, W)\n",
        "    # Bounding Boxes: Tensor(B, N, 5)\n",
        "    H = images.shape[2]\n",
        "    B = images.shape[0] \n",
        "    new_images = []\n",
        "    new_bounding_boxes = []\n",
        "    new_classification = []\n",
        "    for b in range(B):\n",
        "      if random.random() > self.p:\n",
        "        base_image = images[b] # (C, H, W) \n",
        "        base_bbox = bounding_boxes[b] # (N, 5)\n",
        "        base_classification = classification[b]\n",
        "        zero_classification = torch.ones_like(base_classification) * DataModule.PAD_BOUNDING_BOXES\n",
        "\n",
        "        concatted_bounding_boxes = torch.cat([base_bbox, base_bbox, base_bbox, base_bbox], dim = 0)\n",
        "        concatted_classification = torch.cat([base_classification, zero_classification, zero_classification, zero_classification])\n",
        "\n",
        "        new_images += [base_image]\n",
        "        new_bounding_boxes += [concatted_bounding_boxes]\n",
        "        new_classification += [concatted_classification]\n",
        "      else:\n",
        "        # Select 3 Other indices to use\n",
        "        idx1 = b\n",
        "        idx2 = random.randint(0, B - 1)\n",
        "        idx3 = random.randint(0, B - 1)\n",
        "        idx4 = random.randint(0, B - 1)\n",
        "\n",
        "        image1 = images[idx1].clone() # (3, H, W)\n",
        "        image2 = images[idx2].clone() # (3, H, W)\n",
        "        image3 = images[idx3].clone() # (3, H, W) \n",
        "        image4 = images[idx4].clone() # (3, H, W)\n",
        "\n",
        "        bbox1 = bounding_boxes[idx1].clone() # (N, 4)\n",
        "        bbox2 = bounding_boxes[idx2].clone() # (N, 4)\n",
        "        bbox3 = bounding_boxes[idx3].clone() # (N, 4)\n",
        "        bbox4 = bounding_boxes[idx4].clone() # (N, 4)\n",
        "\n",
        "        classes1 = classification[idx1].clone()\n",
        "        classes2 = classification[idx2].clone()\n",
        "        classes3 = classification[idx3].clone()\n",
        "        classes4 = classification[idx4].clone()\n",
        "        BORDER = 50\n",
        "        x1 = random.randint(0, H - BORDER)\n",
        "        y1 = random.randint(0, H - BORDER) \n",
        "\n",
        "        corner1 = image1[:, y1:, x1:] # Bottom Right of Image\n",
        "        corner2 = image2[:, :y1, x1:] # Top Right of Image \n",
        "        corner3 = image3[:, y1:, :x1] # Bottom Left of Image\n",
        "        corner4 = image4[:, :y1, :x1] # Top Left of Image\n",
        "        def compute_area(bboxes):\n",
        "          w = torch.clip(bboxes[:, 3] - bboxes[:, 1], min = 0)\n",
        "          h = torch.clip(bboxes[:, 2] - bboxes[:, 0], min = 0)\n",
        "          return w * h \n",
        "        def add_to(bboxes, shift_x, shift_y):\n",
        "          bboxes[:, [0, 2]] = bboxes[:, [0, 2]] + shift_y\n",
        "          bboxes[:, [1, 3]] = bboxes[:, [1, 3]] + shift_x\n",
        "          return bboxes\n",
        "        def clip(bboxes, min_x, max_x, min_y, max_y, classes):\n",
        "          bboxes[:, [0, 2]] = torch.clip(bboxes[:, [0, 2]], min = min_y, max = max_y)\n",
        "          bboxes[:, [1, 3]] = torch.clip(bboxes[:, [1, 3]], min = min_x, max = max_x)\n",
        "          area = compute_area(bboxes)\n",
        "          remove = area <= 0 \n",
        "          classes[remove] = DataModule.PAD_BOUNDING_BOXES\n",
        "          return bboxes, classes\n",
        "        # Shift the Bounding Boxes\n",
        "        \n",
        "        bboxes1, classes1 = clip(bbox1, x1, H, y1, H, classes1)  # Bottom Right\n",
        "        bboxes2, classes2= clip(bbox2, x1, H, 0, y1, classes2) # TOp Right\n",
        "        bboxes3, classes3 = clip(bbox3, 0, x1, y1, H, classes3) # Bottom Left\n",
        "        bboxes4, classes4 = clip(bbox4, 0, x1, 0, y1, classes4) # Top Left\n",
        "\n",
        "        bboxes = torch.cat([bboxes1, bboxes2, bboxes3, bboxes4])\n",
        "        classes = torch.cat([classes1, classes2, classes3, classes4])\n",
        "\n",
        "        top_half = torch.cat([corner2, corner1], dim = 1)\n",
        "        bottom_half = torch.cat([corner4, corner3], dim = 1)\n",
        "\n",
        "        full_image = torch.cat([bottom_half, top_half], dim = 2)\n",
        "        new_images += [full_image]\n",
        "        new_bounding_boxes += [bboxes]\n",
        "        new_classification += [classes]\n",
        "    return torch.stack(new_images), torch.stack(new_bounding_boxes), torch.stack(new_classification)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMbFRxL22RVi"
      },
      "source": [
        "class TrainDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, train_data, train_keys):\n",
        "    self.train_data = train_data\n",
        "    self.train_keys = train_keys\n",
        "    \n",
        "    self.augments = data_augmentations\n",
        "    self.convert_bboxes = ConvertToBBoxes()\n",
        "    self.padding_boxes = PadBBoxes()\n",
        "  def __len__(self):\n",
        "    return len(self.train_data)\n",
        "  def __getitem__(self, idx):\n",
        "    key = self.train_keys[idx] \n",
        "    data = self.train_data[key] # (N, 5)\n",
        "    image = cv2.imread(key)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    # Extract away the classes and augment\n",
        "    classes = data[:, -1] # (N,)\n",
        "    bbox_reg = data[:, :-1] # (N, 4)\n",
        "    _, H, _ = image.shape\n",
        " \n",
        "    # Remove Weird Tiny Bounding Boxes with virtually 0 area\n",
        "    keep = (bbox_reg > 0.0).astype(np.int32) \n",
        "    ultikeep = keep[:, 0]\n",
        "    for i in range(keep.shape[1]):\n",
        "      ultikeep = ultikeep * keep[:, i]\n",
        "    keep = ultikeep.astype(np.bool)\n",
        "\n",
        "    bbox_reg = bbox_reg[keep].astype(np.float32)\n",
        "    classes = classes[keep]\n",
        "    # Clip the Bboxes\n",
        "    bbox_reg = np.clip(bbox_reg, a_min = 0, a_max = None)\n",
        "    augmented = self.augments(image = image, bboxes = bbox_reg, classes = classes)\n",
        "    image = torch.tensor(augmented['image']) / 255.0\n",
        "    bbox_reg = torch.tensor(augmented['bboxes'])\n",
        "    classes = torch.tensor(augmented['classes'])\n",
        "\n",
        "    bbox_reg = bbox_reg.view(-1, 4)\n",
        "    classes = classes.view(-1) \n",
        "\n",
        "    bbox_reg = torch.cat([bbox_reg, torch.unsqueeze(classes, dim = -1)], dim = -1) # (B, 5)\n",
        "    bbox_reg, classification = self.convert_bboxes(image, bbox_reg)\n",
        "    # Pad the Bounding Boxes\n",
        "    image, bbox_reg, classification = self.padding_boxes(image, bbox_reg, classification)\n",
        "\n",
        "    # Convert to YXYX\n",
        "    bbox_reg = bbox_reg[:, [1, 0, 3, 2]]\n",
        "    return image.float(), bbox_reg.float(), classification.float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIjTPYrc_LQd"
      },
      "source": [
        "class ValDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, val_data, val_keys):\n",
        "    self.val_data = val_data\n",
        "    self.val_keys = val_keys\n",
        "\n",
        "    self.augmentations = test_augmentations\n",
        "    self.convert_bboxes = ConvertToBBoxes()\n",
        "    self.padding_bboxes = PadBBoxes()\n",
        "  def __len__(self):\n",
        "    return len(self.val_data)\n",
        "  def __getitem__(self, idx):\n",
        "    key = self.val_keys[idx]\n",
        "    data = self.val_data[key]\n",
        "\n",
        "    image = cv2.imread(key)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    classes = data[:, -1] # (N, )\n",
        "    bbox_reg = data[:, :-1] # (N, 4)\n",
        "    # Remove Weird Tiny Bounding Boxes with virtually 0 area\n",
        "    keep = (bbox_reg > 0.0).astype(np.int32) \n",
        "    ultikeep = keep[:, 0]\n",
        "    for i in range(keep.shape[1]):\n",
        "      ultikeep = ultikeep * keep[:, i]\n",
        "    keep = ultikeep.astype(np.bool)\n",
        "\n",
        "    bbox_reg = bbox_reg[keep].astype(np.float32)\n",
        "    classes = classes[keep]\n",
        "    # Clip the Bboxes\n",
        "    bbox_reg = np.clip(bbox_reg, a_min = 0, a_max = None)\n",
        "\n",
        "    augmented = self.augmentations(image = image, bboxes = bbox_reg, classes = classes)\n",
        "    \n",
        "    images = torch.tensor(augmented['image']) / 255.0\n",
        "    bbox_reg = torch.tensor(augmented['bboxes'])\n",
        "    classes = torch.tensor(augmented['classes'])\n",
        "    \n",
        "    bbox_reg = bbox_reg.view(-1, 4)\n",
        "    classes = classes.view(-1, 1)\n",
        "\n",
        "    bboxes = torch.cat([bbox_reg, classes], dim = -1)\n",
        "    bbox_reg, classification = self.convert_bboxes(images, bboxes)\n",
        "    images, bbox_reg, classification = self.padding_bboxes(images, bbox_reg, classification)\n",
        "    # Convert YOLO to Regular BBOX format(EffDet Needs [yxyx format]).\n",
        "    bbox_reg = yxyx_to_xyxy(bbox_reg)\n",
        "    return images.float(), bbox_reg.float(), classification.float() \n",
        "\n",
        "def get_dfs():\n",
        "  train_dataset = TrainDataset(TRAIN_DATA, TRAIN_KEYS)\n",
        "  val_dataset = ValDataset(VAL_DATA, VAL_KEYS)\n",
        "  return train_dataset, val_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sxxij4G2IbFN"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY5yeXzovVkF"
      },
      "source": [
        "def select_only_non_padded_bboxes(bboxes, classification):\n",
        "  indices = classification >= 0.0\n",
        "  return bboxes[indices], classification[indices]\n",
        "def yxyx_to_xyxy(bbox):\n",
        "  bbox = bbox[:, [1, 0, 3, 2]]\n",
        "  return bbox\n",
        "def display_bbox_normal(image, bbox):\n",
        "  for box in bbox:\n",
        "    if box[-1] == 0:\n",
        "      continue\n",
        "    image = image.copy()\n",
        "    cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), 220, 3)\n",
        "  plt.imshow(image)\n",
        "  plt.show()   \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6zco2_NImpK"
      },
      "source": [
        "# Collate Function(With Mosaic and Mixup)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE5vbHNN6eWD"
      },
      "source": [
        "class Collator:\n",
        "  mosaic = Mosaic(p = 1)\n",
        "  mixup = Mixup(p = 0.5) \n",
        "\n",
        "  @classmethod\n",
        "  def train_collate_fn(cls, all_boxes):\n",
        "    images = [ex[0] for ex in all_boxes]\n",
        "    bboxes = [ex[1] for ex in all_boxes]\n",
        "    classification = [ex[2] for ex in all_boxes]\n",
        "\n",
        "    images = torch.stack(images)\n",
        "    bboxes = torch.stack(bboxes)\n",
        "    classification = torch.stack(classification)\n",
        "    \n",
        "    images, bboxes, classification = cls.mixup(images, bboxes, classification)\n",
        "    #images, bboxes, classification = cls.mosaic(images, bboxes, classification) # Mosaic is unneccesary.\n",
        "    return images, bboxes, classification\n",
        "  @classmethod\n",
        "  def val_collate_fn(cls, all_boxes):\n",
        "    images = [ex[0] for ex in all_boxes]\n",
        "    bboxes = [ex[1] for ex in all_boxes]\n",
        "    classification = [ex[2] for ex in all_boxes]\n",
        "\n",
        "    images = torch.stack(images)\n",
        "    bboxes = torch.stack(bboxes)\n",
        "    classification = torch.stack(classification)\n",
        "    return images, bboxes, classification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BizYz11OkSY"
      },
      "source": [
        "# Model Used: EfficientDet-D0 - AdvProp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQm4XGc3ktq1"
      },
      "source": [
        "Download Model Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpD1ppMNkszS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbb4f921-e000-4ca4-a5f8-0f212d6003da"
      },
      "source": [
        "!wget https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_d0_ap-d0cdbd0a.pth"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-26 01:23:22--  https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_d0_ap-d0cdbd0a.pth\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-releases.githubusercontent.com/250391956/1bd07180-a9cc-11eb-9fcd-e30331ecc639?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210626%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210626T012322Z&X-Amz-Expires=300&X-Amz-Signature=fa692e15c529f529fee38b9e5d673931365b937c3996c949663761c5703738e6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=250391956&response-content-disposition=attachment%3B%20filename%3Dtf_efficientdet_d0_ap-d0cdbd0a.pth&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-06-26 01:23:22--  https://github-releases.githubusercontent.com/250391956/1bd07180-a9cc-11eb-9fcd-e30331ecc639?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210626%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210626T012322Z&X-Amz-Expires=300&X-Amz-Signature=fa692e15c529f529fee38b9e5d673931365b937c3996c949663761c5703738e6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=250391956&response-content-disposition=attachment%3B%20filename%3Dtf_efficientdet_d0_ap-d0cdbd0a.pth&response-content-type=application%2Foctet-stream\n",
            "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.108.154, 185.199.109.154, 185.199.110.154, ...\n",
            "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.108.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15839413 (15M) [application/octet-stream]\n",
            "Saving to: ‘tf_efficientdet_d0_ap-d0cdbd0a.pth’\n",
            "\n",
            "tf_efficientdet_d0_ 100%[===================>]  15.11M  62.3MB/s    in 0.2s    \n",
            "\n",
            "2021-06-26 01:23:23 (62.3 MB/s) - ‘tf_efficientdet_d0_ap-d0cdbd0a.pth’ saved [15839413/15839413]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1XAWPhXkv2H"
      },
      "source": [
        "class PyTorchLightningModelWrapper(pl.LightningModule):\n",
        "  def __init__(self, model):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "  def forward(self, *args, **kwargs):\n",
        "    return self.model(*args, **kwargs)\n",
        "IMG_SIZE = 512\n",
        "def get_net():\n",
        "    config = get_efficientdet_config('tf_efficientdet_d0') # Smallest Model Possible, to make it runable on CPU(Especially an Android CPU)\n",
        "    net = EfficientDet(config, pretrained_backbone=False)\n",
        "    checkpoint = torch.load('/content/tf_efficientdet_d0_ap-d0cdbd0a.pth')\n",
        "    net.load_state_dict(checkpoint)\n",
        "    effdet.config.config_utils.set_config_writeable(config) \n",
        "    config.num_classes = DataModule.NUM_CLASSES\n",
        "    config.image_size = (IMG_SIZE, IMG_SIZE)\n",
        "    effdet.config.config_utils.set_config_readonly(config)\n",
        "    net.class_net = HeadNet(config, num_outputs=config.num_classes)\n",
        "    model = DetBenchTrain(net, config)\n",
        "    return PyTorchLightningModelWrapper(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT74rWGqdbY0"
      },
      "source": [
        "# Training Config "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4zY9oJUddAT"
      },
      "source": [
        "class TrainingConfig:\n",
        "  batch_size = 16\n",
        "  shuffle = True\n",
        "  num_workers = 4\n",
        "  pin_memory = True\n",
        "  collate_fn = Collator.val_collate_fn\n",
        "  num_epochs = 1000000\n",
        "\n",
        "  config = {\n",
        "      'batch_size': batch_size,\n",
        "      'shuffle': shuffle,\n",
        "      'num_workers': num_workers,\n",
        "      'pin_memory': pin_memory,\n",
        "      'collate_fn': collate_fn\n",
        "  }\n",
        "class ValidationConfig:\n",
        "  batch_size = 16\n",
        "  shuffle = False\n",
        "  num_workers = 4 \n",
        "  pin_memory = True\n",
        "  collate_fn = Collator.val_collate_fn\n",
        "\n",
        "  config = {\n",
        "      'batch_size': batch_size,\n",
        "      'shuffle': shuffle,\n",
        "      'num_workers': num_workers,\n",
        "      'pin_memory': pin_memory,\n",
        "      'collate_fn': collate_fn\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txff9LtRfLsk"
      },
      "source": [
        "# Instantiate Model Object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL391fvVfPqD"
      },
      "source": [
        "\n",
        "class TRAININGOBJECT(pl.LightningModule):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # ----------------PARAMETERS----------------\n",
        "    train, val = get_dfs()\n",
        "    self.lr = 1e-5\n",
        "    self.weight_decay = 1e-6\n",
        "    self.max_lr = 1e-4\n",
        "    self.NUM_TRAIN = len(train)\n",
        "    self.NUM_VAL = len(val)\n",
        "    self.steps_per_epoch = len(train) // TrainingConfig.batch_size \n",
        "    self.num_epochs =TrainingConfig.num_epochs\n",
        "    self.total_steps = self.num_epochs * self.steps_per_epoch\n",
        "    # ----------------DEFINE OBJECTS------------\n",
        "    self.model = get_net()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJhXGJejT1bN"
      },
      "source": [
        "# Logger Object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4O7EFTET3kM"
      },
      "source": [
        "class Logger(pl.LightningModule):\n",
        "  def __init__(self, training_object):\n",
        "    super().__init__()\n",
        "    self.training_object = training_object\n",
        "    self.best_loss = float('inf')\n",
        "    self.save_path = '/content/drive/MyDrive/EARTH_Models/'\n",
        "    self.EPOCHS = 0\n",
        "    \n",
        "  def update_states(self, train_loss, val_loss):\n",
        "\n",
        "    if val_loss <= self.best_loss:\n",
        "      print(\"SAVING STATES\")\n",
        "      torch.save(self.training_object.model.model.state_dict(), f\"{self.save_path}best.pth\")\n",
        "      \n",
        "      self.best_loss = val_loss\n",
        "    print(f\"E: {self.EPOCHS}, BL: {self.best_loss}, TL: {train_loss}, VL: {val_loss}\")\n",
        "    self.EPOCHS += 1    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVfyAgFZcOg3"
      },
      "source": [
        "# Create Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJNcsSCfdXxM"
      },
      "source": [
        "Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egsj6mnlt5hx"
      },
      "source": [
        "def train_step(images, bboxes, classification, training_object):\n",
        "  # Images: Tensor(B, 3, 512, 512)\n",
        "  # Bboxes: Tensor(B, 100, 4)\n",
        "  # Classes: Tensor(B, 100)\n",
        " \n",
        "  outputs = training_object.model(images, target = {'bbox': bboxes, 'cls': classification})\n",
        "  # Grab the Loss\n",
        "  loss = outputs['loss']\n",
        "\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2TysSdtdYrs"
      },
      "source": [
        "Val Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86Syt8Y6dMHi"
      },
      "source": [
        "def val_step(images, bboxes, classification, training_object):\n",
        "  # Images: Tensor(B, 3, 512, 512)\n",
        "  # Bboxes: tensor(B, 100, 4)\n",
        "  # Classes: Tensor(B, 100)\n",
        "  # Run the model, grab the predictions for later(We use Loss as Eval Metric, but you could compute MAP if you wanted - too slow)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = training_object.model(images, target = {\n",
        "      'bbox': bboxes.float(), \n",
        "      'cls': classification, \n",
        "      'img_scale': torch.tensor([1.0] * TrainingConfig.batch_size, dtype=torch.float).to(device), \n",
        "      'img_size':  torch.tensor([images[0].shape[-2:]] * TrainingConfig.batch_size, \n",
        "      dtype=torch.float).to(device)\n",
        "    })\n",
        "  loss = outputs['loss']\n",
        "  return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE6colutbwvf"
      },
      "source": [
        "# Pytorch Lightning Based Training Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6YYeH4OdWtW"
      },
      "source": [
        "class PyTorchLightningModel(pl.LightningModule):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.model = self.configure_model()\n",
        "\n",
        "    self.logger_object = Logger(self.model)\n",
        "\n",
        "\n",
        "    self.train_loss = 0.0\n",
        "    self.train_steps = 0\n",
        "    self.val_loss = 0.0\n",
        "    self.val_steps = 0\n",
        "  def reset_states(self):\n",
        "    self.train_loss, self.train_steps, self.val_loss, self.val_steps = [0] * 4 \n",
        "  def forward(self, images):\n",
        "    pass # No Need for Forward Method, never used at Train \n",
        "  def configure_model(self):\n",
        "    model = TRAININGOBJECT()\n",
        "    return model \n",
        "  def configure_optimizers(self):\n",
        "    optimizer = optim.AdamW(self.logger_object.training_object.model.parameters(), \n",
        "        lr = self.logger_object.training_object.lr,\n",
        "        weight_decay = self.logger_object.training_object.weight_decay)\n",
        "    \n",
        "    lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 5, eta_min = 1e-13, verbose = True)\n",
        "    return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler}\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    images, bboxes, classification = batch\n",
        "    images = images.half()\n",
        "    loss = train_step(images, bboxes, classification, training_object=self.logger_object.training_object)\n",
        "    self.train_loss += loss.item()\n",
        "    self.train_steps += 1\n",
        "    return loss\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    images, bboxes, classification = batch\n",
        "    images = images.half()\n",
        "    loss = val_step(images, bboxes, classification, training_object=self.logger_object.training_object)\n",
        "    self.val_loss += loss.item()\n",
        "    self.val_steps += 1\n",
        "  def validation_epoch_end(self, logs):\n",
        "    eps = 1e-10\n",
        "    self.logger_object.update_states((self.train_loss + eps) / (self.train_steps + eps), (self.val_loss + eps) / (self.val_steps + eps))\n",
        "    self.train_loss = 0.\n",
        "    self.train_steps = 0.\n",
        "    self.val_loss = 0.\n",
        "    self.val_steps = 0.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1XfnSpc5osv"
      },
      "source": [
        "# Training Loop. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLwd31cxb1M1"
      },
      "source": [
        "def TRAIN_MODEL(model_path):\n",
        "  model = PyTorchLightningModel()\n",
        "  if model_path:\n",
        "    model.model.model.model.load_state_dict(torch.load(model_path, map_location = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')))\n",
        "  train, val = get_dfs()\n",
        "  train_dataloader = torch.utils.data.DataLoader(dataset = train, **TrainingConfig.config)\n",
        "  val_dataloader = torch.utils.data.DataLoader(dataset =  val, **ValidationConfig.config)\n",
        "  \n",
        "  # Callbacks\n",
        "  cbs = []\n",
        "  # Create Trainer Object\n",
        "  trainer = pl.Trainer(\n",
        "      gpus = 1,\n",
        "      precision = 16,\n",
        "      overfit_batches = 0.0,\n",
        "      check_val_every_n_epoch = 1,\n",
        "      callbacks = cbs,\n",
        "      gradient_clip_val = 20.0,\n",
        "      max_epochs =TrainingConfig.num_epochs,\n",
        "      profiler = None, \n",
        "      checkpoint_callback = False,\n",
        "      benchmark = True,\n",
        "      deterministic = False,\n",
        "      num_sanity_val_steps = 0,\n",
        "      logger = None\n",
        "  )\n",
        "  trainer.fit(model, train_dataloader, val_dataloader)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laG0l3Cq7xk3"
      },
      "source": [
        "TRAIN_MODEL('/content/drive/MyDrive/EARTH_Models/best.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLKOODENb36_"
      },
      "source": [
        "from effdet.bench import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udB1qNgAY4HV"
      },
      "source": [
        "class InferenceModel(pl.LightningModule):\n",
        "  def __init__(self, prev_model):\n",
        "    super().__init__()\n",
        "    self.prev_model = prev_model \n",
        "    self.model = PyTorchLightningModel()\n",
        "    self.model.model.model.model.load_state_dict(torch.load(self.prev_model, map_location = device))\n",
        "    self.model = DetBenchPredict(self.model.model.model.model.model)\n",
        "  def forward(self, x):\n",
        "    self.eval()\n",
        "    B = x.shape[0]\n",
        "    with torch.no_grad():\n",
        "      x = x.to(device)\n",
        "      predictions =  self.model(x, img_info = {'img_scale': torch.tensor([1.0] * B, dtype=torch.float).to(device), \n",
        "      'img_size':  torch.tensor([x[0].shape[-2:]] * B, \n",
        "      dtype=torch.float).to(device)})\n",
        "      # Hard NMS\n",
        "      idx = torchvision.ops.nms(predictions[0, :, :4], predictions[0, :, 4], iou_threshold = 0.5)\n",
        "      predictions = predictions[0, idx]\n",
        "      return predictions\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDi0PSz0Xt_M"
      },
      "source": [
        "train, val = get_dfs()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elTYiIL_YTn_"
      },
      "source": [
        "model = InferenceModel('/content/drive/MyDrive/EARTH_Models/best.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBlqtSEdYdgx"
      },
      "source": [
        "for images, bboxes, classification in val:\n",
        "  pred = model(images.unsqueeze(0))\n",
        "  #print(classification)\n",
        "  display_preds(images.transpose(0, 1).transpose(1, 2).numpy(), pred.cpu().numpy(), thresh = 0.25)\n",
        "  display_bbox_normal(images.transpose(0, 1).transpose(1, 2).numpy(), yxyx_to_xyxy(bboxes.numpy(), ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-waPsn41YeZ9"
      },
      "source": [
        "def display_preds(image, pred, thresh = 0.2):\n",
        "  for bbox in pred:\n",
        "    x1, y1, x2, y2, obj, cls = bbox\n",
        "    \n",
        "    if obj > thresh:\n",
        "      print(DataModule.idx2classes[cls.item()])\n",
        "      \n",
        "      image = image.copy()\n",
        "      cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), 220, 3)\n",
        "  plt.imshow(image)\n",
        "  plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DpZW4p4fGQn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}